This study investigates the application of model-agnostic, post-hoc, and local interpretability techniques using the SAM (Segmentation Anything Model) segmentation model. Through an experimental approach, we evaluate how incorporating SAM-generated superpixels improves the coherence and understandability of explanations in interpretation methods such as LIME, GLIME, and SEDC. The results highlight a significant improvement in the coherence of explanations, underlining the effectiveness of SAM in generating more relevant and understandable segments. This work highlights the need for future research to explore the applicability of these techniques in specific domains and suggests the implementation of more objective empirical studies to validate the usefulness of the generated explanations.
